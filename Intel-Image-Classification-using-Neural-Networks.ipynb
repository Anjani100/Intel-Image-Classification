{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation_function import sigmoid, relu, sigmoid_backward, relu_backward\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import os, random, cv2, pickle\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Storing the data in a Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = ['train', 'test']\n",
    "DATADIR = \"D:\\Dataset\\intel-image-classification\"\n",
    "CATEGORIES = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "training_data = []\n",
    "testing_data = []\n",
    "IMG_SIZE = 150\n",
    "\n",
    "for s in sets:\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, str(\"seg_\" + s), str(\"seg_\" + s), category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img = os.path.join(path, img)\n",
    "                img_array = cv2.imread(img)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                if s == \"train\":\n",
    "                    training_data.append([new_array, class_num])\n",
    "                elif s == \"test\":\n",
    "                    testing_data.append([new_array, class_num])\n",
    "#                 plt.imshow(new_array)  # To visualise the image\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(testing_data)\n",
    "print(len(training_data), len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "y_train_list = []\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for feature, label in training_data:\n",
    "    X_train_list.append(feature)\n",
    "    y_train_list.append(label)\n",
    "\n",
    "for feature, label in testing_data:\n",
    "    X_test_list.append(feature)\n",
    "    y_test_list.append(label)\n",
    "\n",
    "X_train_orig = np.array(X_train_list)\n",
    "y_train_orig = np.array(y_train_list, ndmin = 2)\n",
    "X_test_orig = np.array(X_test_list)\n",
    "y_test_orig = np.array(y_test_list, ndmin = 2)\n",
    "\n",
    "print(X_train_orig.shape, y_train_orig.shape, X_test_orig.shape, y_test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out_X = open('X_train.pickle', 'wb')\n",
    "pickle.dump(X_train_orig, pickle_out_X)\n",
    "pickle_out_X.close()\n",
    "\n",
    "pickle_out_y = open('y_train.pickle', 'wb')\n",
    "pickle.dump(y_train_orig, pickle_out_y)\n",
    "pickle_out_y.close()\n",
    "\n",
    "pickle_out_X = open('X_test.pickle', 'wb')\n",
    "pickle.dump(X_test_orig, pickle_out_X)\n",
    "pickle_out_X.close()\n",
    "\n",
    "pickle_out_y = open('y_test.pickle', 'wb')\n",
    "pickle.dump(y_test_orig, pickle_out_y)\n",
    "pickle_out_y.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('X_train.pickle', 'rb')\n",
    "X_train_orig = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open('y_train.pickle', 'rb')\n",
    "y_train_orig = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open('X_test.pickle', 'rb')\n",
    "X_test_orig = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open('y_test.pickle', 'rb')\n",
    "y_test_orig = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 14034\n",
      "Number of testing examples: 3000\n",
      "Each image is of size: (150, 150, 3)\n",
      "X_train_orig shape: (14034, 150, 150, 3)\n",
      "y_train_orig shape: (1, 14034)\n",
      "X_test_orig shape: (3000, 150, 150, 3)\n",
      "y_test_orig shape: (1, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Exploring the dataset\n",
    "\n",
    "print('Number of training examples:', X_train_orig.shape[0])\n",
    "print('Number of testing examples:', X_test_orig.shape[0])\n",
    "print('Each image is of size: (' + str(X_train_orig.shape[1]) + \", \" + str(X_train_orig.shape[2]) + \", \" + str(X_train_orig.shape[3]) + \")\")\n",
    "print('X_train_orig shape:', X_train_orig.shape)\n",
    "print('y_train_orig shape:', y_train_orig.shape)\n",
    "print('X_test_orig shape:', X_test_orig.shape)\n",
    "print('y_test_orig shape:', y_test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14034, 67500)\n",
      "X_test shape: (3000, 67500)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the training and testing examples\n",
    "\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1)\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1)\n",
    "\n",
    "# Normalizing the data\n",
    "\n",
    "X_train = X_train_flatten / 255.0\n",
    "X_test = X_test_flatten / 255.0\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the weights and biases for a 2-layer neural network\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "        n_x - Size of input layer\n",
    "        n_h - Size of hidden layer\n",
    "        n_y - Size of output layer\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random_randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the weights and biases for an L-layer neural network\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1])\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the basic forward propagation algorithm: Z = W.T + b for a 2-layer Neural Network\n",
    "\n",
    "def linear_forward(W, A, b):\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the necessary activation function to Z (received from linear_forward function)\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(W, A_prev, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(W, A_prev, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing forward propagation for an L-layer Neural Network\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \n",
    "    L = len(parameters) / 2\n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"W\" + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the cost function\n",
    "\n",
    "def compute_cost(AL, y):\n",
    "    \n",
    "    m = y.shape[1]        # number of examples\n",
    "    cost = - (1 / m) * np.sum(np.multiply(np.log(AL), y) + np.multiply(np.log(1 - AL), (1 - y)))\n",
    "    \n",
    "    cost = np.squeeze(cost)  # Checking the shape of cost; Converts [[21]] to 21\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the backward propagation algorithm:\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis = 1, keepdims = True)     # keepdims = True changes a vector of dimension (n,) to (n, 1)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
